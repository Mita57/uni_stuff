title;keywords;annotation;class
A novel approach to fuzzy clustering based on a dissimilarity relation extracted from data using a TS system;"fuzzy clustering; fuzzy identification; similarity relation;";Clustering refers to the process of unsupervised partitioning of a data set based on a dissimilarity measure, which determines the cluster shape. Considering that cluster shapes may change from one cluster to another, it would be of the utmost importance to extract the dissimilarity measure directly from the data by means of a data model. On the other hand, a model construction requires some kind of supervision of the data structure, which is exactly what we look for during clustering. So, the lower the supervision degree used to build the data model, the more it makes sense to resort to a data model for clustering purposes. Conscious of this, we propose to exploit very few pairs of patterns with known dissimilarity to build a TS system which models the dissimilarity relation. Among other things, the rules of the TS system provide an intuitive description of the dissimilarity relation itself. Then we use the TS system to build a dissimilarity matrix which is fed as input to an unsupervised fuzzy relational clustering algorithm, denoted any relation clustering algorithm (ARCA), which partitions the data set based on the proximity of the vectors containing the dissimilarity values between each pattern and all the other patterns in the data set. We show that combining the TS system and the ARCA algorithm allows us to achieve high classification performance on a synthetic data set and on two real data sets. Further, we discuss how the rules of the TS system represent a sort of linguistic description of the dissimilarity relation.;1
A mixture of experts committee machine to design compensators for intensity modulated radiation therapy;"committee machines; neural networks; compensators; radiation therapy;";"This paper presents a new algorithm to produce a near optimal mixture of experts model (MEM) architecture for a continuous mapping. The MEM is applied to a new method incorporating photon scatter for designing compensators for intensity modulated radiation therapy. The algorithm utilizes the fuzzy C-means clustering algorithm to partition data before training commences. A reduction in the size of training sets also allows the Levenberg&ndash;Marquardt algorithm to be implemented. As a result, both training time and validation error are reduced. A 71% reduction in prediction error compared with that of a single neural network is achieved.";1
Recognizing facial action units using independent component analysis and support vector machine;"facial expression recognition; action unit; independent component analysis; support vector machine;";"Facial expression provides a crucial behavioral measure for studies of human emotion, cognitive processes, and social interaction. In this paper, we focus on recognizing facial action units (AUs), which represent the subtle change of facial expressions. We adopt ICA (independent component analysis) as the feature extraction and representation method and SVM (support vector machine) as the pattern classifier. By comparing with three existing systems, such as Tian, Donato, and Bazzo, our proposed system can achieve the highest recognition rates. Furthermore, the proposed system is fast since it takes only 1.8&#xA0;ms for classifying a test image.";1
Efficient bottom-up hybrid hierarchical clustering techniques for protein sequence classification;"hybrid clustering; hierarchical structure; protein sequences; median strings/sequences; prototypes; feature selection; classification accuracy;";"Hybrid hierarchical clustering techniques which combine the characteristics of different partitional clustering techniques or partitional and hierarchical clustering techniques are interesting. In this paper, efficient bottom-up hybrid hierarchical clustering (BHHC) techniques have been proposed for the purpose of prototype selection for protein sequence classification. In the first stage, an incremental partitional clustering technique such as leader algorithm (ordered leader no update (OLNU) method) which requires only one database (db) scan is used to find a set of subcluster representatives. In the second stage, either a hierarchical agglomerative clustering (HAC) scheme or a partitional clustering algorithm&mdash;&lsquo;K-medians&rsquo; is used on these subcluster representatives to obtain a required number of clusters. Thus, this hybrid scheme is scalable and hence would be suitable for clustering large data sets and we also get a hierarchical structure consisting of clusters and subclusters and the representatives of which are used for pattern classification. Even if more number of prototypes are generated, classification time does not increase much as only a part of the hierarchical structure is searched. The experimental results (classification accuracy (CA) using the prototypes obtained and the computation time) of the proposed algorithms are compared with that of the hierarchical agglomerative schemes, K-medians and nearest neighbour classifier (NNC) methods. The proposed methods are found to be computationally efficient with reasonably good CA.";1
Clustering techniques for protein surfaces;"clustering; region growing; template matching; protein surface;";Though most approaches to protein comparison are based on their structure, several studies produced evidence of a strict correlation between the surface characteristics of proteins and the way they interact. Surface-based techniques for protein comparison typically require applying clustering algorithms to the punctual 3D description of the surface in order to produce a compact surface representation, capable of effectively condensing its description. In this paper, we propose a formalization of the requirements for surface clustering in the biochemical context and present two different clustering techniques that meet them, based, respectively, on region-growing and on an original template matching algorithm. We discuss the validity of these techniques with the support of tests performed on a set of about one hundred protein models generated by punctual mutations of four structurally characterized proteins. Finally, an analysis is made of how different factors impact on the effectiveness of clustering in capturing surface similarities.;1
Feature dimensionality reduction for the verification of handwritten numerals;"dimensionality reduction]; hybrid feature extraction; handwritten character recognition and verification; discriminant analysis; artificial neural networks; ocr;";"A novel method based on multi-modal discriminant analysis is proposed to reduce feature di-mensionality. First, each class is divided into several clusters by thek-means algorithm. The optimal discriminant analysis is implemented by multi-modal mapping. Our method utilizes only those training samples on and near the effective decision boundary to generate a between-class scatter matrix, which requires less CPU time than other nonparametric discriminant analysis (NDA) approaches [Fukunaga and Mantock in IEEE Trans PAMI 5(6):671_677, 1983; Bressan and Vitria in Pattern Recognit Lett 24(5):2473_2749, 2003]. In addition, no prior assumptions about class and cluster densities are needed. In order to achieve a high verification performance of confusing handwritten numeral pairs, a hybrid feature extraction scheme is developed, which consists of a set of gradient-based wavelet features and a set of geometric features. Our proposed dimensionality reduction algorithm is used to congregate features, and it outperforms the principal component analysis (PCA) and other NDA approaches. Experiments proved that our proposed method could achieve a high feature compression performance without sacrificing its discriminant ability for classification. As a result, this new method can reduce artificial neural network (ANN) training complexity and make the ANN classifier more reliable.";1
Breast cancer diagnosis using genetic programming generated feature;"feature extraction; genetic programming; fisher discriminant analysis; pattern recognition;";This paper proposes a novel method for breast cancer diagnosis using the feature generated by genetic programming (GP). We developed a new feature extraction measure (modified Fisher linear discriminant analysis (MFLDA)) to overcome the limitation of Fisher criterion. GP as an evolutionary mechanism provides a training structure to generate features. A modified Fisher criterion is developed to help GP optimize features that allow pattern vectors belonging to different categories to distribute compactly and disjoint regions. First, the MFLDA is experimentally compared with some classical feature extraction methods (principal component analysis, Fisher linear discriminant analysis, alternative Fisher linear discriminant analysis). Second, the feature generated by GP based on the modified Fisher criterion is compared with the features generated by GP using Fisher criterion and an alternative Fisher criterion in terms of the classification performance. The classification is carried out by a simple classifier (minimum distance classifier). Finally, the same feature generated by GP is compared with a original feature set as the inputs to multi-layer perceptrons and support vector machine. Results demonstrate the capability of this method to transform information from high-dimensional feature space into one-dimensional space and automatically discover the relationship among data, to improve classification accuracy.;1
A prototype classification method and its use in a hybrid solution for multiclass pattern recognition;"fuzzy c-means clustering algorithm; handwritten character recognition; hybrid classifier; k-means clustering algorithm; prototype learning; support vector machine;";In this paper, we propose a prototype classification method that employs a learning process to determine both the number and the location of prototypes. This learning process decides whether to stop adding prototypes according to a certain termination condition, and also adjusts the location of prototypes using either the K-means (KM) or the fuzzy c-means (FCM) clustering algorithms. When the prototype classification method is applied, the support vector machine (SVM) method can be used to post-process the top-rank candidates obtained during the prototype learning or matching process. We apply this hybrid solution to handwriting recognition and address the convergence behavior and runtime consumption of the prototype construction process, and discuss how to combine our prototype classifier with SVM classifiers to form an effective hybrid classifier.;1
Visual learning and recognition of 3D objects using two-dimensional principal component analysis: A robust and an efficient approach;"principal component analysis; appearance based model; object recognition;";"Inspired by the conviction that the successful model employed for face recognition [M. Turk, A. Pentland, Eigenfaces for recognition, J. Cogn. Neurosci. 3(1) (1991) 71&ndash;86] should be extendable for object recognition [H. Murase, S.K. Nayar, Visual learning and recognition of 3-D objects from appearance, International J. Comput. Vis. 14(1) (1995) 5&ndash;24], in this paper, a new technique called two-dimensional principal component analysis (2D-PCA) [J. Yang et al., Two-dimensional PCA: a new approach to appearance based face representation and recognition, IEEE Trans. Patt. Anal. Mach. Intell. 26(1) (2004) 131&ndash;137] is explored for 3D object representation and recognition. 2D-PCA is based on 2D image matrices rather than 1D vectors so that the image matrix need not be transformed into a vector prior to feature extraction. Image covariance matrix is directly computed using the original image matrices, and its eigenvectors are derived for feature extraction. The experimental results indicate that the 2D-PCA is computationally more efficient than conventional PCA (1D-PCA) [H. Murase, S.K. Nayar, Visual learning and recognition of 3-D objects from appearance, International J. Comput. Vis. 14(1) (1995) 5&ndash;24]. It is also revealed through experimentation that the proposed method is more robust to noise and occlusion.";1
A robust method for detecting facial orientation in infrared images;"infrared images; facial orientation estimation; single linkage clustering; outlier detection;";This paper studies the problem of determining facial orientation without correspondences in distortion-related infrared images. To improve estimation accuracy and reduce sensitivity to noise and unavoidable error, a simple and robust method based on the single linkage clustering is proposed to simultaneously detect inlier set and estimate orientation angle under contaminated data. An iterative strategy is adopted to avoid random choice of link distance in the single linkage clustering. The experimental results indicate that the proposed method is substantially superior to the moment method. This method can also be extended to detect arbitrary objects with mirror symmetrical or nearly symmetrical property.;1
Ensemble of HMM classifiers based on the clustering validity index for a handwritten numeral recognizer;"hidden markov models; ensemble of classifiers; codebook size; clustering validity index; pattern recognition;";A new scheme for the optimization of codebook sizes for Hidden Markov Models (HMMs) and the generation of HMM ensembles is proposed in this paper. In a discrete HMM, the vector quantization procedure and the generated codebook are associated with performance degradation. By using a selected clustering validity index, we show that the optimization of HMM codebook size can be selected without training HMM classifiers. Moreover, the proposed scheme yields multiple optimized HMM classifiers, and each individual HMM is based on a different codebook size. By using these to construct an ensemble of HMM classifiers, this scheme can compensate for the degradation of a discrete HMM.;1
Distributional-based texture classification using non-parametric statistics;"texture classification; non-parametric distance; distributional metric; statistical graph matching; texture features;";Texture classification is an important problem in image analysis. In the present study, an efficient strategy for classifying texture images is introduced and examined within a distributional-statistical framework. Our approach incorporates the multivariate Wald_Wolfowitz test (WW-test), a non-parametric statistical test that measures the similarity between two different sets of multivariate data, which is utilized here for comparing texture distributions. By summarizing the texture information using standard feature extraction methodologies, the similarity measure provides a comprehensive estimate of the match between different images based on graph theory. The proposed _distributional metric_ is shown to handle efficiently the texture-space dimensionality and the limited sample size drawn from a given image. The experimental results, from the application on a typical texture database, clearly demonstrate the effectiveness of our approach and its superiority over other well-established texture distribution (dis)similarity metrics. In addition, its performance is used to evaluate several approaches for texture representation. Even though the classification results are obtained on grayscale images, a direct extension to color-based ones can be straightforward.;1
Extraction and optimization of fuzzy association rules using multi-objective genetic algorithm;"fuzzy association rules; multi-objective genetic algorithms; fuzzy k-means clustering;";Association Rule Mining is one of the important data mining activities and has received substantial attention in the literature. Association rule mining is a computationally and I/O intensive task. In this paper, we propose a solution approach for mining optimized fuzzy association rules of different orders. We also propose an approach to define membership functions for all the continuous attributes in a database by using clustering techniques. Although single objective genetic algorithms are used extensively, they degenerate the solution. In our approach, extraction and optimization of fuzzy association rules are done together using multi-objective genetic algorithm by considering the objectives such as fuzzy support, fuzzy confidence and rule length. The effectiveness of the proposed approach is tested using computer activity dataset to analyze the performance of a multi processor system and network audit data to detect anomaly based intrusions. Experiments show that the proposed method is efficient in many scenarios.;1
Rotation-Invariant Texture Retrieval via Signature Alignment Based on Steerable Sub-Gaussian Modeling;"feature extraction; image retrieval; image texture; decomposition levels; feature extraction; fractional lower order moments; matrix-based norm; orientation subbands; rotation-invariance property; rotation-invariant texture retrieval; signature alignment; similarity measurement; steerable multivariate sub-gaussian model; steerable pyramid; computer science; feature extraction; image databases; image retrieval; information retrieval; iron; performance evaluation; rotation measurement; samarium; spatial databases; fractional lower-order moments (flom); rotation-invariant texture retrieval; steerable multivariate sub-gaussian model; algorithms; artificial intelligence; computer simulation; image enhancement; image interpretation, computer-assisted; models, statistical; normal distribution; pattern recognition, automated; rotation; subtraction technique;""";This paper addresses the construction of a novel efficient rotation-invariant texture retrieval method that is based on the alignment in angle of signatures obtained via a steerable sub-Gaussian model. In our proposed scheme, we first construct a steerable multivariate sub-Gaussian model, where the fractional lower-order moments of a given image are associated with those of its rotated versions. The feature extraction step consists of estimating the so-called covariations between the orientation subbands of the corresponding steerable pyramid at the same or at adjacent decomposition levels and building an appropriate signature that can be rotated directly without the need of rotating the image and recalculating the signature. The similarity measurement between two images is performed using a matrix-based norm that includes a signature alignment in angle between the images being compared, achieving in this way the desired rotation-invariance property. Our experimental results show how this retrieval scheme achieves a lower average retrieval error, as compared to previously proposed methods having a similar computational complexity, while at the same time being competitive with the best currently known state-of-the-art retrieval system. In conclusion, our retrieval method provides the best compromise between complexity and average retrieval performance.;1
Entropy-Coded Lattice Vector Quantization Dedicated to the Block Mixture Densities;"gaussian processes; data compression; edge detection; entropy codes; image coding; image segmentation; image texture; pattern clustering; rate distortion theory; vector quantisation; wavelet transforms; block mixture density; codebook; dead zone lattice vector quantizers; entropy-coded lattice vector quantization; image compression; image edge; image texture; independent identically distributed generalized gaussian source; joint vector distribution; multidimensional mixture of generalized gaussian density; rate-distortion tradeoff; thresholding vector; vector clustering; wavelet coding; wavelet domain; analytical rate-distortion model; lattice vector quantization (lvq); mixture of gamma densities; mixture of generalized gaussian densities (mggd); sparsity; vector dead zone; algorithms; computer simulation; data compression; image enhancement; image interpretation, computer-assisted; models, statistical; reproducibility of results; sensitivity and specificity; signal processing, computer-assisted;""";Entropy-coded lattice vector quantization (ECLVQ) with codebooks dedicated to independent identically distributed (i.i.d.) generalized Gaussian sources have proven their high coding performances in the wavelet domain. It is well known that wavelet coefficients with high magnitude (corresponding to edges and textures) tend to be clustered in a few amount of vectors. In this paper, we first show that this property has a major influence on the performances of ECLVQ schemes. Since this clustering property cannot be taken into account by the classical i.i.d. assumption, our first proposal is to model the joint distribution of vectors by a multidimensional mixture of generalized Gaussian (MMGG) densities. The main outcome of this MMGG model is to provide a theoretical framework to simply derive from i.i.d. - models, the corresponding MMGG - models. In a second part, a new codebook better suited to wavelet coding is proposed: the so-called dead zone lattice vector quantizers (DZLVQ). It consists of generalizing the scalar dead zone to vector quantization by thresholding vectors according to their energy. We show that DZLVQ improves the rate-distortion tradeoff. Experimental results are provided for the pyramidal LVQ scheme under the assumption of a multidimensional mixture of Laplacian (MML) densities. Results performed on a set of real life images show the precision of the analytical - curves and the efficiency of the DZLVQ scheme.;1
A Hybrid Color and Frequency Features Method for Face Recognition;"face recognition; feature extraction; frequency-domain analysis; image classification; image colour analysis; cff; efm; enhanced fisher model; face recognition; feature extraction; frequency domain; hybrid color-frequency feature method; image classification; application software; color; computer vision; discrete fourier transforms; displays; face recognition; feature extraction; frequency domain analysis; pattern recognition; space technology; enhanced fisher model (efm); face recognition grand challenge (frgc); the &lt;emphasis emphasistype=&#034;boldital&#034;&gt;riq&lt;/emphasis&gt; color space; algorithms; artificial intelligence; biometry; color; colorimetry; face; humans; image enhancement; image interpretation, computer-assisted; pattern recognition, automated; reproducibility of results; sensitivity and specificity;""";This correspondence presents a novel hybrid Color and Frequency Features (CFF) method for face recognition. The CFF method, which applies an Enhanced Fisher Model (EFM), extracts the complementary frequency features in a new hybrid color space for improving face recognition performance. The new color space, the RIQ color space, which combines the R component image of the RGB color space and the chromatic components I and Q of the YIQ color space, displays prominent capability for improving face recognition performance due to the complementary characteristics of its component images. The EFM then extracts the complementary features from the real part, the imaginary part, and the magnitude of the R image in the frequency domain. The complementary features are then fused by means of concatenation at the feature level to derive similarity scores for classification. The complementary feature extraction and feature level fusion procedure applies to the I and Q component images as well. Experiments on the Face Recognition Grand Challenge (FRGC) version 2 Experiment 4 show that i) the hybrid color space improves face recognition performance significantly, and ii) the complementary color and frequency features further improve face recognition performance.;1
A novel and quick SVM-based multi-class classifier;"svm; multi-class classifier; objective function;";Use different real positive numbers pipi to represent all kinds of pattern categories, after mapping the inputted patterns into a special feature space by a non-linear mapping, a linear relation between the mapped patterns and numbers pipi is assumed, whose bias and coefficients are undetermined, and the hyper-plane corresponding to zero output of the linear relation is looked as the base hyper-plane. To determine the pending parameters, an objective function is founded aiming to minimize the difference between the outputs of the patterns belonging to a same type and the corresponding pipi, and to maximize the distance between any two different hyper-planes corresponding to different pattern types. The objective function is same to that of support vector regression in form, so the coefficients and bias of the linear relation are calculated by some known methods such as SVMlightSVMlight approach. Simultaneously, three methods are also given to determine pipi, the best one is to determine them in training process, which has relatively high accuracy. Experiment results of the IRIS data set show that, the accuracy of this method is better than those of many SVM-based multi-class classifiers, and close to that of DAGSVM (decision-directed acyclic graph SVM), emphatically, the recognition speed is the highest.;2
Understanding gestures with systematic variations in movement dynamics;"sign language recognition; gesture recognition; bayesian networks; classifier combination; independent channels; signer adaptation;";Sign language communication includes not only lexical sign gestures but also grammatical processes which represent inflections through systematic variations in sign appearance. We present a new approach to analyse these inflections by modelling the systematic variations as parallel channels of information with independent feature sets. A Bayesian network framework is used to combine the channel outputs and infer both the basic lexical meaning and inflection categories. Experiments using a simulated vocabulary of six basic signs and five different inflections (a total of 20 distinct gestures) obtained from multiple test subjects yielded 85.0% recognition accuracy. We also propose an adaptation scheme to extend a trained system to recognize gestures from a new person by using only a small set of data from the new person. This scheme yielded 88.5% recognition accuracy for the new person while the unadapted system yielded only 52.6% accuracy.;2
Compression and recognition of dance gestures using a deformable model;"contemporary dance; data compression; deformable model; gesture recognition; hidden markov model; motion capture;";In this paper, we aim for the recognition of a set of dance gestures from contemporary ballet. Our input data are motion trajectories followed by the joints of a dancing body provided by a motion-capture system. It is obvious that direct use of the original signals is unreliable and expensive. Therefore, we propose a suitable tool for non-uniform sub-sampling of spatiotemporal signals. The key to our approach is the use of a deformable model to provide a compact and efficient representation of motion trajectories. Our dance gesture recognition method involves a set of hidden Markov models (HMMs), each of them being related to a motion trajectory followed by the joints. The recognition of such movements is then achieved by matching the resulting gesture models with the input data via HMMs. We have validated our recognition system on 12 fundamental movements from contemporary ballet performed by four dancers.;2
Nonlinear approximation based image recovery using adaptive sparse reconstructions and iterated denoising-part II: adaptive algorithms;"adaptive signal processing; image denoising; image reconstruction; image resolution; iterative methods; mean square error methods; transforms; adaptive sparse reconstructions; image recovery; image texture; iterated methods; linear transform; nonlinear approximation; nonstationary signals; progressive algorithm; adaptive algorithm; algorithm design and analysis; approximation algorithms; data models; engineering profession; image edge detection; image reconstruction; image segmentation; noise reduction; signal processing; error concealment; image recovery; inpainting; iterated denoising; nonlinear approximation; sparse recovery; sparse representations; algorithms; artificial intelligence; computer graphics; computer simulation; image enhancement; image interpretation, computer-assisted; imaging, three-dimensional; information storage and retrieval; models, statistical; nonlinear dynamics; pattern recognition, automated; reproducibility of results; sensitivity and specificity; signal processing, computer-assisted;""";We combine the main ideas introduced in Part I with adaptive techniques to arrive at a powerful algorithm that estimates missing data in nonstationary signals. The proposed approach operates automatically based on a chosen linear transform that is expected to provide sparse decompositions over missing regions such that a portion of the transform coefficients over missing regions are zero or close to zero. Unlike prevalent algorithms, our method does not necessitate any complex preconditioning, segmentation, or edge detection steps, and it can be written as a progression of denoising operations. We show that constructing estimates based on nonlinear approximants is fundamentally a nonconvex problem and we propose a progressive algorithm that is designed to deal with this issue directly. The algorithm is applied to images through an extensive set of simulation examples, primarily on missing regions containing textures, edges, and other image features that are not readily handled by established estimation and recovery methods. We discuss the properties required of good transforms, and in conjunction, show the types of regions over which well-known transforms provide good predictors. We further discuss extensions of the algorithm where the utilized transforms are also chosen adaptively, where unpredictable signal components in the progressions are identified and not predicted, and where the prediction scenario is more general.;3
Comparison between immersion-based and toboggan-based watershed image segmentation;"image segmentation; immersion-based watershed image segmentation; order-invariant algorithms; toboggan-based watershed image segmentation; biomedical imaging; computer science; computer vision; councils; electronic mail; image color analysis; image segmentation; information science; inspection; testing; immersion approach; order-invariance; toboggan approach; watershed image segmentation; algorithms; image enhancement; image interpretation, computer-assisted; imaging, three-dimensional; information storage and retrieval; pattern recognition, automated; signal processing, computer-assisted;""";"Watershed segmentation has recently become a popular tool for image segmentation. There are two approaches to implementing watershed segmentation: immersion approach and toboggan simulation. Conceptually, the immersion approach can be viewed as an approach that starts from low altitude to high altitude and the toboggan approach as an approach that starts from high altitude to low altitude. The former seemed to be more popular recently (e.g., Vincent and Soille), but the latter had its own supporters (e.g., Mortensen and Barrett). It was not clear whether the two approaches could lead to exactly the same segmentation result and which approach was more efficient. In this paper, we present two ""order-invariant"" algorithms for watershed segmentation, one based on the immersion approach and the other on the toboggan approach. By introducing a special RIDGE label to achieve the property of order-invariance, we find that the two conceptually opposite approaches can indeed obtain the same segmentation result. When running on a Pentium-III PC, both of our algorithms require only less than 1/30 s for a 256 &times; 256 image and 1/5 s for a 512 &times; 512 image, on average. What is more surprising is that the toboggan algorithm, which is less well known in the computer vision community, turns out to run faster than the immersion algorithm for almost all the test images we have used, especially when the image is large, say, 512 &times; 512 or larger. This paper also gives some explanation as to why the toboggan algorithm can be more efficient in most cases.";3
A modified SPIHT algorithm for image coding with a joint MSE and classification distortion measure;"data compression; hidden markov models; image classification; image coding; image segmentation; mean square error methods; quantisation (signal); bayes tree-structured vector quantization; lagrangian distortion; mse; classification distortion; hidden markov tree; image coding; image-recognition; kernel matching pursuits method; mean-squared error; modified spiht algorithm; set partitioning in hierarchical trees algorithm; textural segmentation; wavelet-based progressive image-compression technique; algorithm design and analysis; bit rate; decoding; distortion measurement; hidden markov models; image coding; kernel; matching pursuit algorithms; partitioning algorithms; wavelet coefficients; classification; hidden markov tree (hmt); image segmentation; set partitioning in hierarchical trees (spiht); vector quantization (vq); algorithms; computer communication networks; data compression; image enhancement; image interpretation, computer-assisted; least-squares analysis; markov chains; models, statistical; reproducibility of results; sensitivity and specificity; signal processing, computer-assisted;""";The set partitioning in hierarchical trees (SPIHT) algorithm is an efficient wavelet-based progressive image-compression technique, designed to minimize the mean-squared error (MSE) between the original and decoded imagery. However, the MSE-based distortion measure is not in general well correlated with image-recognition quality, especially at low bit rates. Specifically, low-amplitude wavelet coefficients that may be important for classification are given low priority by conventional SPIHT. In this paper, we use the kernel matching pursuits (KMP) method to autonomously estimate the importance of each wavelet subband for distinguishing between different textures, with textural segmentation first performed via a hidden Markov tree. Based on subband importance determined via KMP, we scale the wavelet coefficients prior to SPIHT coding, with the goal of minimizing a Lagrangian distortion based jointly on the MSE and classification error. For comparison we consider Bayes tree-structured vector quantization (B-TSVQ), also designed to obtain a tradeoff between MSE and classification error. The performances of the original SPIHT, the modified SPIHT, and B-TSVQ are compared.;3
Space-time image sequence analysis: object tunnels and occlusion volumes;"hidden feature removal; image motion analysis; image segmentation; image sequences; object detection; motion detection; object detection; object tunnels; occlusion volumes; space time image sequence analysis; spatiotemporal segmentation; volume competition; active contours; image analysis; image segmentation; image sequence analysis; image sequences; motion detection; motion estimation; object detection; performance analysis; spatiotemporal phenomena; active surfaces; level set methods; motion detection; video segmentation; volume competition; algorithms; artificial intelligence; image enhancement; image interpretation, computer-assisted; imaging, three-dimensional; information storage and retrieval; motion; pattern recognition, automated; subtraction technique; time factors; video recording;""";"We address the issue of image sequence analysis jointly in space and time. While typical approaches to such an analysis consider two image frames at a time, we propose to perform this analysis jointly over multiple frames. We concentrate on spatiotemporal segmentation of image sequences and on analysis of occlusion effects therein. The segmentation process is three-dimensional (3-D); we search for a volume carved out by each moving object in the image sequence domain, or ""object tunnel"", a new space-time concept. We pose the problem in variational framework by using only motion information (no intensity edges). The resulting formulation can be viewed as volume competition, a 3-D generalization of region competition. We parameterize the unknown surface to be estimated, but rather than using an active-surface approach, we embed it into a higher dimensional function and apply the level-set methodology. We first develop simple models for the detection of moving objects over static background; no motion models are needed. Then, in order to improve segmentation accuracy, we incorporate motion models for objects and background. We further extend the method by including explicit models for occluded and newly exposed areas that lead to ""occlusion volumes,"" another new space-time concept. Since, in this case, multiple volumes are sought, we apply a multiphase variant of the level-set method. We present various experimental results for synthetic and natural image sequences.";3
Detecting fingerprint minutiae by run length encoding scheme;"fingerprint; minutiae; feature extraction; minutiae extraction; run length encoding;";Many approaches to minutiae extraction have already been proposed for automatic fingerprint matching, and most transform fingerprint images into binary images through state-of-the-art algorithms and submit the binary image to a thinning process. However, this paper proposes an original technique for extracting minutiae based on representing the ridge structure of a fingerprint image as a run length code (RLC). The essential idea is to detect minutiae by searching for the termination points or bifurcation points of ridges in the RLC, rather than in a fingerprint image. Experimental results and a comparative analysis show that the proposed method is fairly reliable and faster than a conventional thinning-based method.;3
Overlapping and multi-touching text-line segmentation by Block Covering analysis;"block covering; text-line segmentation; overlapping and multi-touching lines; block counting; ancient arabic documents;";This paper presents a new approach for text-line segmentation based on Block Covering which solves the problem of overlapping and multi-touching components. Block Covering is the core of a system which processes a set of ancient Arabic documents from historical archives. The system is designed for separating text-lines even if they are overlapping and multi-touching. We exploit the Block Covering technique in three steps: a new fractal analysis (Block Counting) for document classification, a statistical analysis of block heights for block classification and a neighboring analysis for building text-lines. The Block Counting fractal analysis, associated with a fuzzy C-means scheme, is performed on document images in order to classify them according to their complexity: tightly (closely) spaced documents (TSD) or widely spaced documents (WSD). An optimal Block Covering is applied on TSD documents which include overlapping and multi-touching lines. The large blocks generated by the covering are then segmented by relying on the statistical analysis of block heights. The final labeling into text-lines is based on a block neighboring analysis. Experimental results provided on images of the Tunisian Historical Archives reveal the feasibility of the Block Covering technique for segmenting ancient Arabic documents.;3
Semi-supervised discriminative classification with application to tumorous tissues segmentation of MR brain images;"magnetic resonance imaging (mri); brain tumor segmentation; semi-automated segmentation; gaussian random field (grf); gaussian process (gp);";Due to the large data size of 3D MR brain images and the blurry boundary of the pathological tissues, tumor segmentation work is difficult. This paper introduces a discriminative classification algorithm for semi-automated segmentation of brain tumorous tissues. The classifier uses interactive hints to obtain models to classify normal and tumor tissues. A non-parametric Bayesian Gaussian random field in the semi-supervised mode is implemented. Our approach uses both labeled data and a subset of unlabeled data sampling from 2D/3D images for training the model. Fast algorithm is also developed. Experiments show that our approach produces satisfactory segmentation results comparing to the manually labeled results by experts.;3
A multi-scale template method for shape detection with bio-medical applications;"non-parametric deformable template; non-affine transformations; image segmentation; dynamic breast imaging; ultrasonography; computed tomography; pet;";In this paper we present a novel methodology based on non-parametric deformable prototype templates for reconstructing the outline of a shape from a degraded image. Our method is versatile and fast and has the potential to provide an automatic procedure for classifying pathologies. We test our approach on synthetic and real data from a variety of medical and biological applications. In these studies it is important to reconstruct accurately the shape of the object under investigation from very noisy data. Here we assume that we have some prior knowledge about the object outline represented by a prototype shape. Our procedure deforms this shape by means of non-affine transformations and the contour is reconstructed by minimizing a newly developed objective function that depends on the transformation parameters. We introduce an iterative template deformation procedure in which the scale of the deformation decreases as the algorithm proceeds. We compare our results with those from a Gaussian Mixture Model segmentation and two state-of-the-art Level Set methods. This comparison shows that the proposed procedure performs consistently well on both real and simulated data. As a by-product we develop a new filter that recovers the connectivity of a shape.;3
Fusion of textural statistics using a similarity measure: application to texture recognition and segmentation;"non-parametric feature statistics; feature fusion and selection; texture recognition; mrf-based texture segmentation;";Features computed as statistics (e.g. histograms) of local filter responses have been reported as the most powerful descriptors for texture classification and segmentation. The selection of the filter banks remains however a crucial issue, as well as determining a relevant combination of these descriptors. To cope with selection and fusion issues, we propose a novel approach relying on the definition of the texture-based similarity measure as a weighted sum of the Kullback_Leibler measures between empirical feature statistics. Within a supervised framework, the weighting factors are estimated according to the maximization of a margin-based criterion. This weighting scheme can also be considered as a filter selection method: texture filter response distributions are ranked according to the associated weighting factors so that the problem of selecting a subset of filters reduces to picking the first features only. An application of this similarity measure to texture recognition is reported. We also investigate its use for texture segmentation within a Bayesian Markov Random Field (MRF)-based framework. Experiments carried out on Brodatz textures and sonar images show that the proposed weighting method improves the classification and the segmentation rates while relying on a parsimonious texture representation.;3
Dynamic Denoising of Tracking Sequences;"bayes methods; kalman filters; estimation theory; image denoising; image enhancement; image sequences; object detection; wavelet transforms; bayesian wavelet estimation; kalman filter; dynamic image denoising; image recovery; image sequence enhancement; object tracking; bayesian estimation; kalman filtering; predictive tracking; wavelet denoising; algorithms; artifacts; image enhancement; image interpretation, computer-assisted; motion; pattern recognition, automated; reproducibility of results; sensitivity and specificity;""";"In this paper, we describe an approach to the problem of simultaneously enhancing image sequences and tracking the objects of interest represented by the latter. The enhancement part of the algorithm is based on Bayesian wavelet denoising, which has been chosen due to its exceptional ability to incorporate diverse a priori information into the process of image recovery. In particular, we demonstrate that, in dynamic settings, useful statistical priors can come both from some reasonable assumptions on the properties of the image to be enhanced as well as from the images that have already been observed before the current scene. Using such priors forms the main contribution of the present paper which is the proposal of the dynamic denoising as a tool for simultaneously enhancing and tracking image sequences. Within the proposed framework, the previous observations of a dynamic scene are employed to enhance its present observation. The mechanism that allows the fusion of the information within successive image frames is Bayesian estimation, while transferring the useful information between the images is governed by a Kalman filter that is used for both prediction and estimation of the dynamics of tracked objects. Therefore, in this methodology, the processes of target tracking and image enhancement ""collaborate"" in an interlacing manner, rather than being applied separately. The dynamic denoising is demonstrated on several examples of SAR imagery. The results demonstrated in this paper indicate a number of advantages of the proposed dynamic denoising over ""static"" approaches, in which the tracking images are enhanced independently of each other.";3
A Variational Method for Geometric Regularization of Vascular Segmentation in Medical Images;"biomedical mri; geometry; image segmentation; medical image processing; set theory; 2d retinal angiogram images; anisotropic propagation; level-set-based geometric regularization method; magnetic resonance angiography volumetric data sets; medical images; shape induced information; variational method; vascular segmentation; vessel segmentation; anisotropic propagation; blood vessel segmentation; energy optimization; shape analysis; surface evolution; algorithms; artificial intelligence; fluorescein angiography; humans; image enhancement; image interpretation, computer-assisted; pattern recognition, automated; reproducibility of results; retinal vessels; retinoscopy; sensitivity and specificity;""";In this paper, a level-set-based geometric regularization method is proposed which has the ability to estimate the local orientation of the evolving front and utilize it as shape induced information for anisotropic propagation. We show that preserving anisotropic fronts can improve elongations of the extracted structures, while minimizing the risk of leakage. To that end, for an evolving front using its shape-offset level-set representation, a novel energy functional is defined. It is shown that constrained optimization of this functional results in an anisotropic expansion flow which is useful for vessel segmentation. We have validated our method using synthetic data sets, 2-D retinal angiogram images and magnetic resonance angiography volumetric data sets. A comparison has been made with two state-of-the-art vessel segmentation methods. Quantitative results, as well as qualitative comparisons of segmentations, indicate that our regularization method is a promising tool to improve the efficiency of both techniques.;3
Automatic Active Model Initialization via Poisson Inverse Gradient;"gradient methods; image segmentation; stochastic processes; pig initialization method; poisson inverse gradient; automatic active model initialization; image processing applications; image segmentation; rapid convergence; superior noise robustness; active contours; poisson inverse gradient; poisson&#039;s equation; active models; active surfaces; deformable models; deformable surfaces; initialization; snakes; algorithms; artificial intelligence; computer simulation; image enhancement; image interpretation, computer-assisted; imaging, three-dimensional; models, statistical; pattern recognition, automated; poisson distribution; reproducibility of results; sensitivity and specificity;""";Active models have been widely used in image processing applications. A crucial stage that affects the ultimate active model performance is initialization. This paper proposes a novel automatic initialization approach for parametric active models in both 2-D and 3-D. The PIG initialization method exploits a novel technique that essentially estimates the external energy field from the external force field and determines the most likely initial segmentation. Examples and comparisons with two state-of-the-art automatic initialization methods are presented to illustrate the advantages of this innovation, including the ability to choose the number of active models deployed, rapid convergence, accommodation of broken edges, superior noise robustness, and segmentation accuracy.;3
Cost Aggregation and Occlusion Handling With WLS in Stereo Matching;"computational complexity; computer graphics; costing; image matching; image resolution; interpolation; iterative methods; least squares approximations; stereo image processing; gauss-seidel method; adaptive interpolation; cost aggregation; energy function; iterative equation; occlusion handling; optimal cost estimation; per-pixel difference image; stereo matching; weighted least squares; cost aggregation; multiscale approach; occlusion handling; stereo vision; weighted least square; algorithms; artificial intelligence; image enhancement; image interpretation, computer-assisted; imaging, three-dimensional; information storage and retrieval; least-squares analysis; models, statistical; pattern recognition, automated; photogrammetry; reproducibility of results; sensitivity and specificity;""";This paper presents a novel method for cost aggregation and occlusion handling for stereo matching. In order to estimate optimal cost, given a per-pixel difference image as observed data, we define an energy function and solve the minimization problem by solving the iterative equation with the numerical method. We improve performance and increase the convergence rate by using several acceleration techniques such as the Gauss-Seidel method, the multiscale approach, and adaptive interpolation. The proposed method is computationally efficient since it does not use color segmentation or any global optimization techniques. For occlusion handling, which has not been performed effectively by any conventional cost aggregation approaches, we combine the occlusion problem with the proposed minimization scheme. Asymmetric information is used so that few additional computational loads are necessary. Experimental results show that performance is comparable to that of many state-of-the-art methods. The proposed method is in fact the most successful among all cost aggregation methods based on standard stereo test beds.;3
Minimization of Region-Scalable Fitting Energy for Image Segmentation;"approximation theory; curve fitting; edge detection; image motion analysis; image segmentation; minimisation; set theory; contour motion; curve evolution equation; data fitting energy; image intensity approximation; image segmentation; kernel function; level set regularization; real-world image intensity inhomogeneity; region-based active contour model; region-scalable fitting energy minimization; variational level set formulation; active contours; active shape model; data mining; equations; image edge detection; image segmentation; kernel; level set; mathematics; robustness; image segmentation; intensity inhomogeneity; level set method; region-scalable fitting energy; variational method; algorithms; artificial intelligence; image enhancement; image interpretation, computer-assisted; pattern recognition, automated; reproducibility of results; sensitivity and specificity;""";Intensity inhomogeneities often occur in real-world images and may cause considerable difficulties in image segmentation. In order to overcome the difficulties caused by intensity inhomogeneities, we propose a region-based active contour model that draws upon intensity information in local regions at a controllable scale. A data fitting energy is defined in terms of a contour and two fitting functions that locally approximate the image intensities on the two sides of the contour. This energy is then incorporated into a variational level set formulation with a level set regularization term, from which a curve evolution equation is derived for energy minimization. Due to a kernel function in the data fitting term, intensity information in local regions is extracted to guide the motion of the contour, which thereby enables our model to cope with intensity inhomogeneity. In addition, the regularity of the level set function is intrinsically preserved by the level set regularization term to ensure accurate computation and avoids expensive reinitialization of the evolving level set function. Experimental results for synthetic and real images show desirable performances of our method.;3
Multistage Branch-and-Bound Merging for Planar Surface Segmentation in Disparity Space;"image segmentation; iterative methods; stereo image processing; disparity image; disparity space; iterative split-and-merge framework; maximum variance constraint; multistage branch-and-bound merging; multistage merging estimation; planar surface segmentation; stereo image data; combinatorial optimization; image segmentation; object detection; stereo vision; algorithms; artificial intelligence; image enhancement; image interpretation, computer-assisted; imaging, three-dimensional; pattern recognition, automated; reproducibility of results; sensitivity and specificity;""";An iterative split-and-merge framework for the segmentation of planar surfaces in the disparity space is presented. Disparity of a scene is modeled by approximating various surfaces in the scene to be planar. In the split phase, the number of planar surfaces along with the underlying plane parameters is assumed to be known from the initialization or from the previous merge phase. Based on these parameters, planar surfaces in the disparity image are labeled to minimize the residuals between the actual disparity and the modeled disparity. The labeled planar surfaces are separated into spatially continuous regions which are treated as candidates for the merging that follows. The regions are merged together under a maximum variance constraint while maximizing the merged area. A multistage branch-and-bound algorithm is proposed to carry out this optimization efficiently. Each stage of the branch-and-bound algorithm separates a planar surface from the set of spatially continuous regions. The multistage merging estimates the number of planar surfaces and their labeling. The splitting and the multistage merging is repeated till convergence is reached or satisfactory results are achieved. Experimental results are presented for variety of stereo image data.;3
"A Multiresolution Stochastic Level Set Method for Mumford&#x2013;Shah Image Segmentation";"image resolution; image segmentation; simulated annealing; stochastic programming; mumford-shah image segmentation; basin hopping; multiresolution stochastic level set; region splitting; simulated annealing; stochastic methods; stochastic optimization; computational efficiency; computational modeling; heart; image resolution; image segmentation; level set; optimization methods; simulated annealing; spatial resolution; stochastic processes; basin hopping; mumford&amp;#x2013;shah model; global optimization; image segmentation; region splitting and merging; stochastic level set method; algorithms; artificial intelligence; computer simulation; image enhancement; image interpretation, computer-assisted; models, statistical; pattern recognition, automated; reproducibility of results; sensitivity and specificity; stochastic processes;""";The Mumford-Shah model is one of the most successful image segmentation models. However, existing algorithms for the model are often very sensitive to the choice of the initial guess. To make use of the model effectively, it is essential to develop an algorithm which can compute a global or near global optimal solution efficiently. While gradient descent based methods are well-known to find a local minimum only, even many stochastic methods do not provide a practical solution to this problem either. In this paper, we consider the computation of a global minimum of the multiphase piecewise constant Mumford-Shah model. We propose a hybrid approach which combines gradient based and stochastic optimization methods to resolve the problem of sensitivity to the initial guess. At the heart of our algorithm is a well-designed basin hopping scheme which uses global updates to escape from local traps in a way that is much more effective than standard stochastic methods. In our experiments, a very high-quality solution is obtained within a few stochastic hops whereas the solutions obtained with simulated annealing are incomparable even after thousands of steps. We also propose a multiresolution approach to reduce the computational cost and enhance the search for a global minimum. Furthermore, we derived a simple but useful theoretical result relating solutions at different spatial resolutions.;3
A Region Merging Prior for Variational Level Set Image Segmentation;"curve fitting; image segmentation; active contours; curve evolution; image segmentation; level sets; region merging; councils; equations; image segmentation; level set; merging; motion estimation; testing; tracking; active contours; image segmentation; level sets; segmentation prior; algorithms; artificial intelligence; image enhancement; image interpretation, computer-assisted; imaging, three-dimensional; pattern recognition, automated; reproducibility of results; sensitivity and specificity; subtraction technique;""";In current level set image segmentation methods, the number of regions is assumed to known beforehand. As a result, it remains constant during the optimization of the objective functional. How to allow it to vary is an important question which has been generally avoided. This study investigates a region merging prior related to regions area to allow the number of regions to vary automatically during curve evolution, thereby optimizing the objective functional implicitly with respect to the number of regions. We give a statistical interpretation to the coefficient of this prior to balance its effect systematically against the other functional terms. We demonstrate the validity and efficiency of the method by testing on real images of intensity, color, and motion.;3
The solutions of equation-based noise detector for an adaptive median filter;"noise detection; equation; impulse noise; median filter;";"Techniques of noise detection have been widely applied in impulse noise reduction. However, the phenomenon of pixel misclassification is very obvious in high noise density. In order to improve pixel identification, in this paper, the new noise detector is proposed. Based on solutions of equations, an estimated block of every 8&times;88&times;8 block of a noise image is generated. Then, according to relationships between these noise blocks and their estimated blocks, corrupted and uncorrupted pixels are identified. During image filtering, a noise-detection-based adaptive median algorithm is presented. Experimental results show that the proposed filter can well reduce the impulse noise and preserve more details of original images.";4
Improved Bayesian image denoising based on wavelets with applications to electron microscopy;"image denoising; bayesian filtering; wavelets;";"In this work we discuss an improvement of the image-denoising wavelet-based method presented by Bijaoui [Wavelets, Gaussian mixtures and Wiener filtering, Signal Process. 82 (2002) 709&ndash;712]. We show that the parameter estimation step can be replaced by a constrained nonlinear optimization. We propose three different methods to estimate the parameters. As in Bijaouis original article, two of them deal with white noise. We show that the resulting algorithms improve the one originally proposed. Our third method extends the applicability of the denoising algorithm to colored noise. We test our algorithms with images simulating electron microscopy (EM) conditions as well as experimental EM images.";4
Adaptive Bilateral Filter for Sharpness Enhancement and Noise Removal;"adaptive filters; edge detection; image denoising; image enhancement; image restoration; image texture; statistical analysis; adaptive bilateral filter; edge detection; edge profile extraction; histogram; image noise removal; image sharpness enhancement; image texture; slope image restoration; unsharp mask; bilateral filter; de-blurring; noise removal; range filter; sharpness enhancement; slope restoration; algorithms; artifacts; artificial intelligence; computer graphics; image enhancement; image interpretation, computer-assisted; information storage and retrieval; numerical analysis, computer-assisted; pattern recognition, automated; reproducibility of results; sensitivity and specificity; signal processing, computer-assisted;""";In this paper, we present the adaptive bilateral filter (ABF) for sharpness enhancement and noise removal. The ABF sharpens an image by increasing the slope of the edges without producing overshoot or undershoot. It is an approach to sharpness enhancement that is fundamentally different from the unsharp mask (USM). This new approach to slope restoration also differs significantly from previous slope restoration algorithms in that the ABF does not involve detection of edges or their orientation, or extraction of edge profiles. In the ABF, the edge slope is enhanced by transforming the histogram via a range filter with adaptive offset and width. The ABF is able to smooth the noise, while enhancing edges and textures in the image. The parameters of the ABF are optimized with a training procedure. ABF restored images are significantly sharper than those restored by the bilateral filter. Compared with an USM based sharpening method-the optimal unsharp mask (OUM), ABF restored edges are as sharp as those rendered by the OUM, but without the halo artifacts that appear in the OUM restored image. In terms of noise removal, ABF also outperforms the bilateral filter and the OUM. We demonstrate that ABF works well for both natural images and text images.;4
Algorithmic and Architectural Optimizations for Computationally Efficient Particle Filtering;"convex programming; image sequences; optical tracking; particle filtering (numerical methods); pipeline processing; sampling methods; video signal processing; convex program; independent metropolis hastings sampler; nongaussian noise process; nonlinear dynamical system filtering; particle filtering algorithm; pipelined architectural optimization; video sequences; visual tracking; auxillary variable; monte carlo markov chain (mcmc); design methodologies; particle filter; resampling; visual tracking; algorithms; artificial intelligence; computer simulation; data compression; image enhancement; image interpretation, computer-assisted; models, statistical; pattern recognition, automated; reproducibility of results; sensitivity and specificity; signal processing, computer-assisted; video recording;""";In this paper, we analyze the computational challenges in implementing particle filtering, especially to video sequences. Particle filtering is a technique used for filtering nonlinear dynamical systems driven by non-Gaussian noise processes. It has found widespread applications in detection, navigation, and tracking problems. Although, in general, particle filtering methods yield improved results, it is difficult to achieve real time performance. In this paper, we analyze the computational drawbacks of traditional particle filtering algorithms, and present a method for implementing the particle filter using the Independent Metropolis Hastings sampler, that is highly amenable to pipelined implementations and parallelization. We analyze the implementations of the proposed algorithm, and, in particular, concentrate on implementations that have minimum processing times. It is shown that the design parameters for the fastest implementation can be chosen by solving a set of convex programs. The proposed computational methodology was verified using a cluster of PCs for the application of visual tracking. We demonstrate a linear speedup of the algorithm using the methodology proposed in the paper.;4
An Orientation-Selective Orthogonal Lapped Transform;"hartley transforms; data compression; filtering theory; image coding; image denoising; image texture; iterative methods; statistical analysis; 2d block hartley transform; biorthogonal transform; cosine wave; image compression; iterative filter; lapped hartley transform; modulating basis functions; noise removal; orientation-selective orthogonal lapped transform; oriented textures; overlapping basis functions; statistical analysis; synthesis operation; lapped transform; local orientation; transform coding; algorithms; image enhancement; image interpretation, computer-assisted; reproducibility of results; sensitivity and specificity;""";A novel critically sampled orientation-selective orthogonal lapped transform called the lapped Hartley transform (LHT) is derived. In a first step, overlapping basis functions are generated by modulating basis functions of a 2-D block Hartley transform by a cosine wave. To achieve invertibility and orthogonality, an iterative filter is applied as prefilter in the analysis and as postfilter in the synthesis operation, respectively. Alternatively, filtering can be restricted to analysis or synthesis, ending up with a biorthogonal transform (LHT-PR, LHT-PO). A statistical analysis based on a 4000-image data base shows that the LHT and LHT-PO have better redundancy removal properties than other block or lapped transforms. Finally, image compression and noise removal examples are given, showing the advantages of the LHT especially in images containing oriented textures.;4
Noise and Signal Estimation in Magnitude MRI and Rician Distributed Images: A LMMSE Approach;"filtering theory; image denoising; least mean squares methods; magnetic resonance imaging; statistical distributions; lmmse approach; rayleigh model; rician model; image noise filtering; linear minimum mean square error estimator; local sample statistical distribution; magnetic resonance imaging; magnitude mri; probability density function; linear minimum mean square error (lmmse) estimator; mri filtering; rician noise; noise estimation; algorithms; artifacts; brain; data interpretation, statistical; image enhancement; image interpretation, computer-assisted; least-squares analysis; magnetic resonance imaging; models, statistical; reproducibility of results; sensitivity and specificity;""";A new method for noise filtering in images that follow a Rician model-with particular attention to magnetic resonance imaging-is proposed. To that end, we have derived a (novel) closed-form solution of the linear minimum mean square error (LMMSE) estimator for this distribution. Additionally, a set of methods that automatically estimate the noise power are developed. These methods use information of the sample distribution of local statistics of the image, such as the local variance, the local mean, and the local mean square value. Accordingly, the dynamic estimation of noise leads to a recursive version of the LMMSE, which shows a good performance in both noise cleaning and feature preservation. This paper also includes the derivation of the probability density function of several local sample statistics for the Rayleigh and Rician model, upon which the estimators are built.;4
Noniterative Interpolation-Based Super-Resolution Minimizing Aliasing in the Reconstructed Image;"image reconstruction; image resolution; interpolation; iterative methods; mesh generation; splines (mathematics); b-splines; delaunay triangulation; image reconstruction; interpolation problem; inverse problem; iterative methods; low-resolution undersampled images; noniterative interpolation; point spread function; point spread function imaging system; sampling theory; super-resolution minimizing aliasing; data models; image reconstruction; image resolution; image restoration; image sampling; interpolation; sampling methods; signal resolution; spline; strontium; image reconstruction; inverse problem; nonuniform sampling; super-resolution; algorithms; artifacts; image enhancement; image interpretation, computer-assisted; reproducibility of results; sensitivity and specificity; subtraction technique;""";Super-resolution (SR) techniques produce a high-resolution image from a set of low-resolution undersampled images. In this paper, we propose a new method for super-resolution that uses sampling theory concepts to derive a noniterative SR algorithm. We first raise the issue of the validity of the data model usually assumed in SR, pointing out that it imposes a band-limited reconstructed image plus a certain type of noise. We propose a sampling theory framework with a prefiltering step that allows us to work with more general data models and also a specific new method for SR that uses Delaunay triangulation and B-splines to build the super-resolved image. The proposed method is noniterative and well posed. We prove its effectiveness against traditional iterative and noniterative SR methods on synthetic and real data. Additionally, we also prove that we can first solve the interpolation problem and then make the deblurring not only when the motion is translational but also when there are rotations and shifts and the imaging system point spread function (PSF) is rotationally symmetric.;4
Multiresolution Bilateral Filtering for Image Denoising;"channel bank filters; image denoising; image resolution; nonlinear filters; wavelet transforms; image denoising; multiresolution bilateral filtering; nonlinear filter; optimal bilateral filter parameter selection; wavelet filter bank; wavelet thresholding; colored noise; filter bank; filtering; image denoising; image resolution; least squares approximation; low-frequency noise; signal resolution; spatial resolution; temperature sensors; algorithms; artifacts; image enhancement; image interpretation, computer-assisted; imaging, three-dimensional; reproducibility of results; sensitivity and specificity; signal processing, computer-assisted;""";"The bilateral filter is a nonlinear filter that does spatial averaging without smoothing edges; it has shown to be an effective image denoising technique. An important issue with the application of the bilateral filter is the selection of the filter parameters, which affect the results significantly. There are two main contributions of this paper. The first contribution is an empirical study of the optimal bilateral filter parameter selection in image denoising applications. The second contribution is an extension of the bilateral filter: multiresolution bilateral filter, where bilateral filtering is applied to the approximation (low-frequency) subbands of a signal decomposed using a wavelet filter bank. The multiresolution bilateral filter is combined with wavelet thresholding to form a new image denoising framework, which turns out to be very effective in eliminating noise in real noisy images. Experimental results with both simulated and real data are provided.";4
Face recognition from a single image per person: A survey;"face recognition; single training image per person;";"One of the main challenges faced by the current face recognition techniques lies in the difficulties of collecting samples. Fewer samples per person mean less laborious effort for collecting them, lower cost for storing and processing them. Unfortunately, many reported face recognition techniques rely heavily on the size and representative of training set, and most of them will suffer serious performance drop or even fail to work if only one training sample per person is available to the systems. This situation is called &ldquo;one sample per person&rdquo; problem: given a stored database of faces, the goal is to identify a person from the database later in time in any different and unpredictable poses, lighting, etc. from just one image. Such a task is very challenging for most current algorithms due to the extremely limited representative of training sample. Numerous techniques have been developed to attack this problem, and the purpose of this paper is to categorize and evaluate these algorithms. The prominent algorithms are described and critically analyzed. Relevant issues such as data collection, the influence of the small sample size, and system evaluation are discussed, and several promising directions for future research are also proposed in this paper.";1
Techniques for the segmentation of striation patterns;"image segmentation; image texture; anisotropy analysis; forensic science; image processing techniques; striation patterns segmentation; anisotropic magnetoresistance; azimuth; forensics; image databases; image processing; image segmentation; light sources; lighting; qualifications; robustness; anisotropy analysis; forensic science; illumination series; image fusion; oriented patterns; striation patterns; algorithms; forensic sciences; image enhancement; image interpretation, computer-assisted; imaging, three-dimensional; information storage and retrieval; pattern recognition, automated;""";In this contribution, image processing techniques are investigated to robustly segment faint striation patterns from mainly isotropic background areas. Whereas well-known procedures to determine local anisotropy can be applied directly to single images, this paper describes an alternative strategy that uses an illumination series. The series is obtained with a spot illumination whose azimuth is varied systematically. The technique is based on the characteristics of the local contrast with respect to the azimuth of the illumination: Whereas pronounced maxima of the local contrast can be observed when the striae are illuminated perpendicularly, an isotropic background texture shows less distinct maxima for random azimuth angles. The qualification of the approach is demonstrated on the segmentation of faint tool marks in forensic science. Experimental results show that the methodology ensures the correct segmentation of such marks.;3
Classification of image objects in Epo doping control using fuzzy decision tree;"epo doping control; fuzzy decision tree; image segmentation; machine learning classification;";"Erythropoietin (Epo) is a hormone which can be misused as a doping substance. Its detection involves analysis of images containing specific objects (bands), whose position and intensity are critical for doping positivity. Within a research project of the World Anti-Doping Agency (WADA) we are implementing the GASepo software that serves for Epo testing in doping control laboratories worldwide. For identification of the bands we have developed a segmentation procedure based on a sequence of filters. Whereas all true bands are properly segmented, the procedure generates a number of false positives (artefacts). To separate these artefacts we suggested a post-segmentation supervised classification using real-valued geometrical measures of objects. The method is based on a fuzzy modification of Ross Quinlan_s ID3 method, included in the mlf"" software (Machine Learning Framework). It provides a framework that generates fuzzy decision trees, as well as fuzzy sets for input data. Initially used training set of segmented objects has been replaced by a new one prepared by more accurate expertise using the latest release of the GASepo software. The new fuzzy decision trees (FDT) have been generated for a set of five and nine fuzzy sets. The comparison of the results on testing set of segmented objects shows that the classification based on the new FDTs outperforms other classification methods.";3
Localizing Region-Based Active Contours;"feature extraction; image segmentation; statistical analysis; global image statistics; heterogeneous feature profiles; image segmentation; objects segmentation; region-based active contours localization; standard global method; active contours; curve evolution; image segmentation; level set methods; multiregion segmentation; partial differential equations; algorithms; artificial intelligence; image enhancement; image interpretation, computer-assisted; pattern recognition, automated; reproducibility of results; sensitivity and specificity;""";In this paper, we propose a natural framework that allows any region-based segmentation energy to be re-formulated in a local way. We consider local rather than global image statistics and evolve a contour based on local information. Localized contours are capable of segmenting objects with heterogeneous feature profiles that would be difficult to capture correctly using a standard global method. The presented technique is versatile enough to be used with any global region-based active contour energy and instill in it the benefits of localization. We describe this framework and demonstrate the localization of three well-known energies in order to illustrate how our framework can be applied to any energy. We then compare each localized energy to its global counterpart to show the improvements that can be achieved. Next, an in-depth study of the behaviors of these energies in response to the degree of localization is given. Finally, we show results on challenging images to illustrate the robust and accurate segmentations that are possible with this new class of active contour models.;3
Monte-Carlo Sure: A Black-Box Optimization of Regularization Parameters for General Denoising Algorithms;"gaussian noise; monte carlo methods; image denoising; image restoration; mean square error methods; optimisation; risk analysis; monte-carlo sure; stein unbiased risk estimate; black-box optimization; corrupted signal restoration; image-denoising algorithm; mean-squared error; regularization parameter; signal denoising algorithm; white gaussian noise; bayesian methods; biomedical measurements; filtering algorithms; gaussian noise; image restoration; noise measurement; noise reduction; signal restoration; smoothing methods; wiener filter; monte-carlo methods; stein&#039;s unbiased risk estimate (sure); regularization parameter; smoothing splines; total-variation denoising; wavelet denoising; algorithms; artifacts; computer simulation; data interpretation, statistical; image enhancement; image interpretation, computer-assisted; models, statistical; monte carlo method; reproducibility of results; sensitivity and specificity;""";We consider the problem of optimizing the parameters of a given denoising algorithm for restoration of a signal corrupted by white Gaussian noise. To achieve this, we propose to minimize Steins unbiased risk estimate (SURE) which provides a means of assessing the true mean-squared error (MSE) purely from the measured data without need for any knowledge about the noise-free signal. Specifically, we present a novel Monte-Carlo technique which enables the user to calculate SURE for an arbitrary denoising algorithm characterized by some specific parameter setting. Our method is a black-box approach which solely uses the response of the denoising operator to additional input noise and does not ask for any information about its functional form. This, therefore, permits the use of SURE for optimization of a wide variety of denoising algorithms. We justify our claims by presenting experimental results for SURE-based optimization of a series of popular image-denoising algorithms such as total-variation denoising, wavelet soft-thresholding, and Wiener filtering/smoothing splines. In the process, we also compare the performance of these methods. We demonstrate numerically that SURE computed using the new approach accurately predicts the true MSE for all the considered algorithms. We also show that SURE uncovers the optimal values of the parameters in all cases.;4
Semidefinite spectral clustering;"clustering; convex optimization; multi-way graph equipartitioning; semidefinite programming; spectral clustering;";Multi-way partitioning of an undirected weighted graph where pairwise similarities are assigned as edge weights, provides an important tool for data clustering, but is an NP-hard problem. Spectral relaxation is a popular way of relaxation, leading to spectral clustering where the clustering is performed by the eigen-decomposition of the (normalized) graph Laplacian. On the other hand, semidefinite relaxation, is an alternative way of relaxing a combinatorial optimization, leading to a convex optimization. In this paper we employ a semidefinite programming (SDP) approach to the graph equipartitioning for clustering, where sufficient conditions for strong duality hold. The method is referred to as semidefinite spectral clustering, where the clustering is based on the eigen-decomposition of the optimal feasible matrix computed by SDP. Numerical experiments with several data sets, demonstrate the useful behavior of our semidefinite spectral clustering, compared to existing spectral clustering methods.;1
Improved quality of reconstructed images using floating point arithmetic for moment calculation;"geometric moments; zernike moments; pattern recognition; feature extraction; image reconstruction;";Zernike moments which are superior to geometric moments because of their special properties of image reconstruction and immunity to noise, suffer from several discretization errors. These errors lead to poor quality of reconstructed image and wide variations in the numerical values of the moments. The predominant factor, as observed in this paper, is due to the discrete integer implementation of the steps involved in moment calculation. It is shown in this paper that by modifying the algorithms to include discrete float implementation, the quality of the reconstructed image improves significantly and the first-order moment becomes zero. Low-order Zernike moments have been found to be stable under linear transformations while the high-order moments have large variations. The large variations in high-order moments, however, do not greatly affect the quality of the reconstructed image, implying that they should be ignored when numerical values of moments are used as features. The 11 functions based on geometric moments have also been found to be stable under linear transformations and thus these can be used as features. Pixel level analysis of the images has been carried out to strengthen the results.;1
A novel approach to the fast computation of Zernike moments;"zernike moments; fast method; symmetry/anti-symmetry; discrete zernike moments;";"This paper presents a novel approach to the fast computation of Zernike moments from a digital image. Most existing fast methods for computing Zernike moments have focused on the reduction of the computational complexity of the Zernike 1-D radial polynomials by introducing their recurrence relations. Instead, in our proposed method, we focus on the reduction of the complexity of the computation of the 2-D Zernike basis functions. As Zernike basis functions have specific symmetry or anti-symmetry about the x-axis, the y &#xA0;-axis, the origin, and the straight line y=xy=x, we can generate the Zernike basis functions by only computing one of their octants. As a result, the proposed method makes the computation time eight times faster than existing methods. The proposed method is applicable to the computation of an individual Zernike moment as well as a set of Zernike moments. In addition, when computing a series of Zernike moments, the proposed method can be used with one of the existing fast methods for computing Zernike radial polynomials. This paper also presents an accurate form of Zernike moments for a discrete image function. In the experiments, results show the accuracy of the form for computing discrete Zernike moments and confirm that the proposed method for the fast computation of Zernike moments is much more efficient than existing fast methods in most cases.";3
A fuzzy logic approach for detection of video shot boundaries;"temporal video segmentation; shot cut detection; shot-boundary detection; content-based video processing;";Video temporal segmentation is normally the first and important step for content-based video applications. Many features including the pixel difference, colour histogram, motion, and edge information etc. have been widely used and reported in the literature to detect shot cuts inside videos. Although existing research on shot cut detection is active and extensive, it still remains a challenge to achieve accurate detection of all types of shot boundaries with one single algorithm. In this paper, we propose a fuzzy logic approach to integrate hybrid features for detecting shot boundaries inside general videos. The fuzzy logic approach contains two processing modes, where one is dedicated to detection of abrupt shot cuts including those short dissolved shots, and the other for detection of gradual shot cuts. These two modes are unified by a mode-selector to decide which mode the scheme should work on in order to achieve the best possible detection performances. By using the publicly available test data set from Carleton University, extensive experiments were carried out and the test results illustrate that the proposed algorithm outperforms the representative existing algorithms in terms of the precision and recall rates.;3
Fuzzy Cognitive Maps for stereovision matching;"fuzzy cognitive maps; fuzzy clustering; relaxation; fuzzy; stereovision; matching; similarity; smoothness; ordering; epipolar; uniqueness;";This paper outlines a method for solving the stereovision matching problem using edge segments as the primitives. In stereovision matching the following constraints are commonly used: epipolar, similarity, smoothness, ordering and uniqueness. We propose a new matching strategy under a fuzzy context in which such constraints are mapped. The fuzzy context integrates both Fuzzy Clustering and Fuzzy Cognitive Maps. With such purpose a network of concepts (nodes) is designed, each concept represents a pair of primitives to be matched. Each concept has associated a fuzzy value which determines the degree of the correspondence. The goal is to achieve high performance in terms of correct matches. The main findings of this paper are reflected in the use of the fuzzy context that allows building the network of concepts where the matching constraints are mapped. Initially, each concept value is loaded via the Fuzzy Clustering and then updated by the Fuzzy Cognitive Maps framework. This updating is achieved through the influence of the remainder neighboring concepts until a good global matching solution is achieved. Under this fuzzy approach we gain quantitative and qualitative matching correspondences. This method works as a relaxation matching approach and its performance is illustrated by comparative analysis against some existing global matching methods.;1
A study of identical twins_ palmprints for personal verification;"identical twins; palmprint; biometric; personal identification;";Automatic biometric systems based on human characteristics for personal identification have attracted great attention. Their performance highly depends on the distinctive information in the biometrics. Identical twins having the closest genetics-based relationship are expected to have maximum similarity in their biometrics. Classifying identical twins is a challenging problem for some automatic biometric systems. Palmprint has been studied for personal identification for over seven years. Most of the previous research concentrates on algorithm development. In this paper, we systemically examine palmprints from the same DNA for automatic personal identification and to uncover the genetically related palmprint features. The experimental results show that the three principal lines and some portions of weak lines are genetically related features but our palms still contain rich genetically unrelated features for classifying identical twins.;3
Face recognition using common faces method;"face recognition; common vector approach; kernel method;";In this paper, we propose a face recognition method called the commonface by using the common vector approach. A face image is regarded as a summation of a common vector which represents the invariant properties of the corresponding face class, and a difference vector which presents the specific properties of the corresponding face image such as face appearance, pose and expression. Thus, by deriving the common vector of each face class, the common feature of each person is obtained which removes the differences of face images belonging to the same person. For test face image, the remaining vector with each face class is derived with the similar procedure to the common vector, which is then compared with the common vector of each face class to predict the class label of query face by finding the minimum distance between the remaining vector and the common vector. Furthermore, we extend the common vector approach (CVP) to kernel CVP to improve the performance of CVP. The experimental results suggest that the proposed commonface approach provides a better representation of individual common feature and achieves lower error rates in face recognition.;3
Fuzzy discriminant analysis with kernel methods;"fuzzy discriminant analysis; kernel methods; kernel fuzzy discriminant analysis;";A novel fuzzy nonlinear classifier, called kernel fuzzy discriminant analysis (KFDA), is proposed to deal with linear non-separable problem. With kernel methods KFDA can perform efficient classification in kernel feature space. Through some nonlinear mapping the input data can be mapped implicitly into a high-dimensional kernel feature space where nonlinear pattern now appears linear. Different from fuzzy discriminant analysis (FDA) which is based on Euclidean distance, KFDA uses kernel-induced distance. Theoretical analysis and experimental results show that the proposed classifier compares favorably with FDA.;1
